cloudera 
* certification
    - The leader in Apache Hadoop-based certification
    - Cloudera certification exams favor hands-on, performance-based problems 
        that require execution of a set of real-world tasks against a live, working cluster
    - Type
        > CCA
            CCA Spark and Hadoop Developer
            CCA Data Analyst
            CCA CDH Administrator and CCA HDP Administrator
        > CCP
            CCP Data Engineer
    
Introduction to Apache Hadoop and the Hadoop Ecosystem
    - 3V
        Volume, Velocity, Variety

Data Processing
    - Processing 
        apache Spark, Hadoop MapReduce
    - Resource Management
        YARN
    - Storage
        HDFS, HBase, Kudu, Cloud

Data Storage
    - HDFS
        main on-premises storage layer for Hadoop
        Inexpensive reliable storage for massive amounts of data
    - Apache HBase
        A NoSQL distributed database built on HDFS
    - Apache Kudu
        key-value storage
    - Cloud storage
        most hadoop ecosystem tools support in the cloude
        ex. Amazon S3, MS ADLS

Hadoop MapReduce: The Original Hadoop Processing Engine
    - the Original Hadoop framework for processing big data 
        Primarily Java-based
    - Still in use in many production systems

<<Apache Hadoop File Storage>>
* Hadoop Cluster Terminology
    - cluster is a group of computers working together
    - Master nodes manage distribution of work and data
    - Worker nodes store and process data
    - A daemon is a program running on a  node to provide a service

* HDFS Architecture

    1) Basic Concepts
    - HDFS is the original Hadoop storage system
    - Sits on top of a native file system
    - Provides redundant storage for massive amounts of data
    - HDFS performs best with a “modest” number of large files
    - Files in HDFS are “write once”
    - HDFS is optimized for large, streaming reads of files

    2) File be sotred
    - Data files are split into blocks (default 128MB) which are distributed at load time
    - Actual blocks are stored on cluster worker nodes running the Hadoop HDFS Data Node service
    - Each block is replicated on multiple DataNodes (default 3x)
    - A cluster master node runs the HDFS Name Node service, which stores file metadata
        Often referred to as the NameNode

                       block1    Data Node A
    large Data File -> block2 -> Data Node B  -> NameNode (Meta Info. about files and blocks)
                       block3    Data Node C
                       block4    Data Node D

* Using HDFS
    - Command line interface
        $hdfs dfs
    
*Essential Points
    - The Hadoop Distributed File System (HDFS) is the main storage layer for Hadoop
    - HDFS chunks data into blocks and distributes them to the cluster when data is stored
    - HDFS clusters consist of
        ─ A single NameNode to manage file metadata
        ─ Multiple DataNodes to store data
    - The hdfs dfs command allows you to use and manage files in HDFS

<<Distributed Processing on an Apache Hadoop Cluster>>

* YARN Architecture
    - What is YARN?
        - YARN = Yet Another Resource Negotiator
        - YARN is Hadoop processing layer (contains)
            Resource manager
            A job scheduler
    - YARN Daemons
        - Resource manager (RM)
            runs on master node
            global resource scheduler
            
        - Node manager (NM)
            runs on worker nodes
            communicates with RM
            Manages node resources
            Launches containers
        
        - containers
            allocate a certain amount of resources(memory, CPU cores) on a worker node
            Applications runs in one or more containers
            Applications request containers from RM

        - Application Master (AM)
            One per Application
            framework/application specific
            runs in a container
            request more containers to run application tasks

        -  Each application consists of one or more containers
            The ApplicationMaster runs in one container
            The application’s distributed processes (JVMs) run in other containers
            The processes run in parallel, and are managed by the AM
            The processes are called executors in Apache Spark and tasks in Hadoop MapReduce
        
        - Applications are typically submitted to the cluster from an edge or gateway node



