cloudera 
* certification
    - The leader in Apache Hadoop-based certification
    - Cloudera certification exams favor hands-on, performance-based problems 
        that require execution of a set of real-world tasks against a live, working cluster
    - Type
        > CCA
            CCA Spark and Hadoop Developer
            CCA Data Analyst
            CCA CDH Administrator and CCA HDP Administrator
        > CCP
            CCP Data Engineer
    
Introduction to Apache Hadoop and the Hadoop Ecosystem
    - 3V
        Volume, Velocity, Variety

Data Processing
    - Processing 
        apache Spark, Hadoop MapReduce
    - Resource Management
        YARN
    - Storage
        HDFS, HBase, Kudu, Cloud

Data Storage
    - HDFS
        main on-premises storage layer for Hadoop
        Inexpensive reliable storage for massive amounts of data
    - Apache HBase
        A NoSQL distributed database built on HDFS
    - Apache Kudu
        key-value storage
    - Cloud storage
        most hadoop ecosystem tools support in the cloude
        ex. Amazon S3, MS ADLS

Hadoop MapReduce: The Original Hadoop Processing Engine
    - the Original Hadoop framework for processing big data 
        Primarily Java-based
    - Still in use in many production systems

<<Apache Hadoop File Storage>>
* Hadoop Cluster Terminology
    - cluster is a group of computers working together
    - Master nodes manage distribution of work and data
    - Worker nodes store and process data
    - A daemon is a program running on a  node to provide a service

* HDFS Architecture

    1) Basic Concepts
    - HDFS is the original Hadoop storage system
    - Sits on top of a native file system
    - Provides redundant storage for massive amounts of data
    - HDFS performs best with a “modest” number of large files
    - Files in HDFS are “write once”
    - HDFS is optimized for large, streaming reads of files

    2) File be sotred
    - Data files are split into blocks (default 128MB) which are distributed at load time
    - Actual blocks are stored on cluster worker nodes running the Hadoop HDFS Data Node service
    - Each block is replicated on multiple DataNodes (default 3x)
    - A cluster master node runs the HDFS Name Node service, which stores file metadata
        Often referred to as the NameNode

                       block1    Data Node A
    large Data File -> block2 -> Data Node B  -> NameNode (Meta Info. about files and blocks)
                       block3    Data Node C
                       block4    Data Node D

* Using HDFS
    - Command line interface
        $hdfs dfs
    
*Essential Points
    - The Hadoop Distributed File System (HDFS) is the main storage layer for Hadoop
    - HDFS chunks data into blocks and distributes them to the cluster when data is stored
    - HDFS clusters consist of
        ─ A single NameNode to manage file metadata
        ─ Multiple DataNodes to store data
    - The hdfs dfs command allows you to use and manage files in HDFS

<<Distributed Processing on an Apache Hadoop Cluster>>

* YARN Architecture
    - What is YARN?
        - YARN = Yet Another Resource Negotiator
        - YARN is Hadoop processing layer (contains)
            Resource manager
            A job scheduler
    - YARN Daemons
        - Resource manager (RM)
            runs on master node
            global resource scheduler
            
        - Node manager (NM)
            runs on worker nodes
            communicates with RM
            Manages node resources
            Launches containers
        
        - containers
            allocate a certain amount of resources(memory, CPU cores) on a worker node
            Applications runs in one or more containers
            Applications request containers from RM

        - Application Master (AM)
            One per Application
            framework/application specific
            runs in a container
            request more containers to run application tasks

        -  Each application consists of one or more containers
            The ApplicationMaster runs in one container
            The application’s distributed processes (JVMs) run in other containers
            The processes run in parallel, and are managed by the AM
            The processes are called executors in Apache Spark and tasks in Hadoop MapReduce
        
        - Applications are typically submitted to the cluster from an edge or gateway node

* Essential Points
    - YARN manages resources in a Hadoop cluster and schedules Applications
    - Worker nodes run NodeManafer daemons, managed by a ResourceManager on a master node
    - Applications running on YARN consist of an ApplicationMaster and one or more executors
    - Use the YARN ResourceManager web UI or the yarn command to monitor Applications

<<Apache Spark Basics>>

* What is Apache Spark?
    - a fast, general-purpose engine for large-scale data processing
    - written in Scala
        Functional programming language that runs in a JVM
    - Spark Shell
        Interactive—for learning, data exploration, or ad hoc analytics
        Python and Scala
    - Spark Applications
        For large scale data processing
        Python, Scala, and Java
    
    - The Spark Stack
        ` Spark provides a stack of libraries built on core Spark
            ┌─────────┐ ┌─────────┐ ┌─────────┐ ┌─────────┐
            │  Spark  │ │   MLib  │ │  Spark  │ │  GraphX │ 
            │   SQL   │ │         │ │Streaming│ │         │
            └─────────┘ └─────────┘ └─────────┘ └─────────┘
            ┌─────────────────────────────────────────────┐
            │                 Core Spark                  │
            └─────────────────────────────────────────────┘
            Core Spark provides the fundamental Spark abstraction:
                Resilient Distributed Datasets (RDDs)
            Spark SQL works with structured data
            MLlib supports scalable machine learning
            Spark Streaming applications process data in real time
            GraphX works with graphs and graph-parallel computation
    
    - Spark SQL
        ` Spark SQL is a SPark library for working with structured data
        ` Spark SQL provide
            The DataFrame and Dataset API
                The primary entry point for developing Spark applications
                DataFrames adnd Datasets are abstractions for representing structured data
            Datalyst Optimizer - an extensible optimization framework
            A SQL engine and command line interface

* The Spark shell
    - The Spark shell provides an interactive Spark environment
    - typically run the Spark shell on a gateway node
    - on Cloudera cluster, the command to start Spark shell
        ` pyspark 
        ` spark-shell
    - start-up options
        ` master : specify the cluster to connect to
        ` jars : Additional JAR files
        ` py-files : Additional Python files (Python only)
        ` name : the name the Spark Application UI uses for this application
            Defaults to PySpark Shell or Spark shell
        ` help: Show all the available shell options
        ex) pyspark --name "My Application"

    - Spark Cluster options
        ` Spark applications can run on these types of clusters
            ─ Apache Hadoop YARN
            ─ Kubernetes
            ─ Apache Mesos
            ─ Spark Standalone
        ` They can also run locally instead of on a cluster
        ` CDH, HDP, and CDP Data Center use YARN
        ` CDP Private Cloud and CDP Public Cloud use Kubernetes
        ` Specify the type or URL of the cluster using the master option
        ` Set the master option to specify cluster type
            ─ yarn
            ─ local[*] runs locally with as many threads as cores (default)
            ─ local[n] runs locally with n threads
            ─ local runs locally with a single thread
        ex)
            pyspark --master yarn
            spark-shell --master yarn

    - Apache Zeppelin
        is a web-based notebook approach to interactive data analytics.
        Provides collaborative environment with Python, Scala, SQL, and more.

* Using Spark Shell
    - Working with the Spark Session
        provides functions and attributes to access all of Spark functionally
        ─ sql: execute a Spark SQL query
        ─ catalog: entry point for the Catalog API for managing tables
        ─ read: function to read data from a file or other data source
        ─ conf: object to manage Spark configuration settings
        ─ sparkContext: entry point for core Spark API

* Datasets and DataFrames
    - DataFrames contain a collection of Row objects
    - Creating DataFrame
        ` use spark.read
            ex) 
            # returns the Spark session's DataFrameReader
            # Call json dunction to create a new DataFrame
            usersDF = spark.read.json("users.json")

            # DataFrame always have an associated schema
            # DataFrameReader can infer the schema from the data
            # Use printSchema to show the DataFrame's schema

            # root
            # |-- age: long (nullable = true)
            # |-- name: string (nullable = true)
            # |-- pcode: string (nullable = true)
            usersDF.printSchema()

            # show
            # +----+-------+-----+
            # | age|   name|pcode|
            # +----+-------+-----+
            # |null|  Alice|94304|
            # |  30|Brayden|94304|
            # |  19|  Carla|10036|
            # |  46|  Diana| null|
            # |null|Etienne|94104|
            # +----+-------+-----+
            usersDF.show()

* DataFrame Operations
    - Two main types of DataFrame Operations
        ` Transformations create a new DataFrame based on existing one 
            Transformations are executed in parallel by the application's executors
        ` Actions output data values from the DataFrame
            Output is typically returned from the executors to the main Spark program (called the driver) or saved to a file
    - Some common DataFrame actions include
        ─ count: returns the number of rows
        ─ first: returns the first row (synonym for head())
        ─ take(n): returns the first n rows as an array (synonym for head(n))
        ─ show(n): display the first n rows in tabular form (default is 20 rows)
        ─ collect: returns all the rows in the DataFrame as an array
        ─ write: save the data to a file or other data source

            # Python ver.
            usersDF = spark.read.json("users.json")
            users = usersDF.take(3)
            
            ### result
            [Row(age=None, name=u'Alice', pcode=u'94304'),
            Row(age=30, name=u'Brayden', pcode=u'94304'),
            Row(age=19, name=u'Carla', pcode=u'10036')]

            # Scala ver.
            val usersDF = spark.read.json("users.json")
            val users = usersDF.take(3)
           
            ### result
            usersDF: Array[org.apache.spark.sql.Row] =
            Array([null,Alice,94304],
            [30,Brayden,94304],
            [19,Carla,10036])

    - DataFrame Operations: Transformations ★★★★★
        ` Transformations create a new DataFrame based on an existing one
            ─ The new DataFrame may have the same schema or a different one
        ` Transformations do not return any values or data to the driver
            ─ Data remains distributed across the application’s executors
        ` DataFrames are immutable
            ─ Data in a DataFrame is never modified
            ─ Use transformations to create a new DataFrame with the data you need  
        ` Common transformations include
            ─ select: only the specified columns are included
            ─ where: only rows where the specified expression is true are included (synonym for filter)
            ─ orderBy: rows are sorted by the specified column(s) (synonym for sort)
            ─ join: joins two DataFrames on the specified column(s)
            ─ limit(n): creates a new DataFrame with only the first n rows

            # Python
            nameAgeDF = usersDF.select("name","age")
            nameAgeDF.show()
            # result
            +-------+----+
            |   name| age|
            +-------+----+
            |  Alice|null|
            |Brayden|  30|
            |  Carla|  19|
            |  Diana|  46|
            |Etienne|null|
            +-------+----+
            over20DF = usersDF.where("age > 20")
            over20DF.show()
            +---+-------+-----+
            |age|   name|pcode|
            +---+-------+-----+
            | 30|Brayden|94304|
            | 46|  Diana| null|
            +---+-------+-----+

        ` A sequence of transformations followed by an action is a query
            nameAgeDF = usersDF.select("name","age")
            nameAgeOver20DF = nameAgeDF.where("age > 20")
            nameAgeOver20DF.show()
            +---+-------+
            |age|   name|
            +---+-------+
            | 30|Brayden|
            | 46|  Diana|
            +---+-------+
    
    *** Chaining Transformations
        ` Transformations in a query can be chained together
        `  These two examples perform the same query in the same way
            ─ Differences are only syntactic

            # Python ver.
            # 1 
            nameAgeDF = usersDF.select("name","age")
            nameAgeOver20DF = nameAgeDF.where("age > 20")
            nameAgeOver20DF.show()
            # 2
            usersDF.select("name","age").where("age > 20").show()

            # Scala ver.
            #1
            val nameAgeDF = usersDF.select("name","age")
            val nameAgeOver20DF = nameAgeDF.where("age > 20")
            nameAgeOver20DF.show            
            #2
            usersDF.select("name","age").where("age > 20").show

* Essential Points
    - Apache Spark is a framework for analyzing and processing big data
    - The Python and Scala Spark shells are command line REPLs for executing Spark interactively
        ─ Spark applications run in batch mode outside the shell
    - DataFrames represent structured data in tabular form by applying a schema
    - Types of DataFrame operations
        ─ Transformations create new DataFrames by transforming data in existing ones
        ─ Actions collect values in a DataFrame and either save them or return them to the Spark driver
    - A query consists of a sequence of transformations followed by an action

<< Working with DataFrames and Schema >>
*  Creating DataFrames from Data Source
    - DataFrame Data Sources
        ` DataFrames read data from and write data to data sources
        ` Spark SQL supports a wide range of data source types and formats
            ─ Text files
                ─ CSV, JSON, plain text
            ─ Binary format files
                ─ Apache Parquet, Apache ORC, Apache Avro data format
            ─ Tables
                ─ Hive metastore, JDBC
            ─ CloudW
                ─ Such as Amazon S3 and Microsoft ADLS
        ` can use custom or third-party data source types
    - DataFrames and Apache Parquet Files
        ` Parquet is a very common file format for DataFrame data
        ` Features of Parquet
            ─ Optimized binary storage of structured data
            ─ Schema metadata is embedded in the file
            ─ Efficient performance and size for large amounts of data
            ─ Supported by many Hadoop ecosystem tools
                ─ Spark, Hadoop MapReduce, Hive, and others
        ` Use parquet-tools to view Parquet file schema and data
            ─ Use head to display the first few records
                $ parquet-tools head mydatafile.parquet
            ─ Use schema to view the schema
                $ parquet-tools schema mydatafile.parquet
    - spark.read return DataFrameReader object
    - use DataFrameReader settings to specify how to load data from the data source
        ` format inducates the data source type, such as csv, json, or parquet (default is parquet)
        ` option specifies a key/value setting for the underlying data source
        ` schema specifies a schema to use instead of inferring one from the data source
    - Create the DataFrame based on the data source
        ` load loads data from a file or files
    - example  
        # read file
        myDF = spark.read.format("csv").option("header","true").load("/loudacre/myFile.csv")
        # == spark.read.csv("/loudacre/myFile.csv")
        # reading dynamical type
        spark.read.json("myfile.json")
        spark.read.json("mydata/)
        spark.read.json("mydata/*.json")
        spark.read.json("myfile1.json","myfile2.json")
    - files and directories are referred by absolute or relative URL
        ` relative URL
        ` absolute URL
    - Apache Hive provides database-like access to data in HDFS
        ` Applies schemas to HDFS files
        ` Metadata is stored in the Hive metastore
    - Spark can read from and write to Hive tables
        ` Infers the DataFrame schema from the Hive metadata
    - Spark Hive support must be enabled and configured with location od the Hive warehouse in HDFS
        usersDF = spark.read.table("users")
    - create DataFrames from a collection of in-memory data
        val mydata = List(("Josiah","Bartlet"),("Harry","Potter"))
        val myDF = spark.createDataFrame(mydata)
        myDF.show()
        # result
        +------+-------+
        |    _1|     _2|
        +------+-------+
        |Josiah|Bartlet|
        | Harry| Potter|
        +------+-------+

* Saving DataFrame to Data Source
    - Key DataFrameWrite functions
        ` Saves data to a data source such as a table or set of files
        ` Works similarly to DataFrameReader
    - DataFrameWrite methods
        ` format specifies a data source type
        ` mode determines the behavior if the directory or table already exists
            ─ error, overwrite, append, or ignore (default is error)
        ` partitionBy stores data in partitioned directories in the form column=value (as with Hive partitioning)
        ` option specifies properties for the target data source
        ` save saves the data as files in the specified directory
            ─ Or use json, csv, parquet, and so on
        ` saveAsTable saves the data to a Hive metastore table
            ─ Data location based on Hive warehouse default
            ─ Set path option to override location
    - example
        myDF.write.mode("append").option("path", "/louadacre/mydata").saveAsTable("my_table")
        # write data as aparquet files in mydata directory
        myDF.write.save("mydata")
        # save data from DataFrame, must specify a directory
            ─ Spark saves the data to one or more part- files in the directory
            devDF.write.csv("devices")
        
            $ hdfs dfs -ls

            devices
            Found 4 items
            -rw-r--r-- 3 training training 0 … devices/_SUCCESS
            -rw-r--r-- 3 training training 2119 … devices/part-00000-e0fa6381-….csv
            -rw-r--r-- 3 training training 2202 … devices/part-00001-e0fa6381-….csv
            -rw-r--r-- 3 training training 2333 … devices/part-00002-e0fa6381-….csv

* DataFrame Schemas
    - Every DataFrame has an associated schema
        ─ Defines the names and types of columns
        ─ Immutable and defined when the DataFrame is created

        myDF.printSchema()
        root
        |-- lastName: string (nullable = true)
        |-- firstName: string (nullable = true)
        |-- age: integer (nullable = true)
    - When creating a new DataFrame from a data source, the schema can be
        ─ Automatically inferred from the data source
        ─ Specified programmatically    
    - When a DataFrame is created by a transformation, Spark calculates the new schema based on the query
    - Inferrd Schemas
        ` Spark can load schemas from structured data, such as
            ─ Parquet, ORC, and Avro data files—schema is embedded in the file
            ─ Hive tables—schema is defined in the Hive metastore
            ─ Parent DataFrames
        ` Spark can also attempt to infer a schema from semi-structured data sources
            ─ For example, JSON and CSV
        ` Example
            # 02134,Hopper,Grace,52
            # 94020,Turing,Alan,32
            # 94020,Lovelace,Ada,28
            # 87501,Babbage,Charles,49
            # 02134,Wirth,Niklaus,48
            # Inferring the Schema of a CSV File (No Header)
            spark.read.option("inferSchema","true").csv("people.csv").printSchema()

            root
            |-- _c0: integer (nullable = true)
            |-- _c1: string (nullable = true)
            |-- _c2: string (nullable = true)
            |-- _c3: integer (nullable = true)

            # pcode,lastName,firstName,age
            # 02134,Hopper,Grace,52
            # 94020,Turing,Alan,32
            # 94020,Lovelace,Ada,28
            # 87501,Babbage,Charles,49
            # 02134,Wirth,Niklaus,48
            # Inferring the Schema of a CSV File (with Header)

            spark.read.option("inferSchema","true").option("header","true").csv("people.csv").printSchema()

            root
            |-- pcode: integer (nullable = true)
            |-- lastName: string (nullable = true)
            |-- firstName: string (nullable = true)
            |-- age: integer (nullable = true)

    - Inferred Schemas versus Manual Schemas
        ` Drawbacks to relying on Spark’s automatic schema inference
            ─ Inference requires an initial file scan, which may take a long time
            ─ The schema may not be correct for your use case
        ` You can define the schema manually instead
            ─ A schema is a StructType object containing a list of StructField objects
            ─ Each StructField represents a column in the schema, specifying
                ─ Column name
                ─ Column data type
                ─ Whether the data can be null (optional—the default is true)

    - Defining Schema Programmatically
        # Python
        from pyspark.sql.types import *
        columnsList = [
        StructField("pcode", StringType()),
        StructField("lastName", StringType()),
        StructField("firstName", StringType()),
        StructField("age", IntegerType())]
        peopleSchema = StructType(columnsList)

        # Scala
        import org.apache.spark.sql.types._
        val columnsList = List(
        StructField("pcode", StringType),
        StructField("lastName", StringType),
        StructField("firstName", StringType),
        StructField("age", IntegerType))
        val peopleSchema = StructType(columnsList)

    - Applying a Schema manually
        spark.read.option("header","true").schema(peopleSchema).csv("people.csv").printSchema()

        root
        |-- pcode: string (nullable = true)
        |-- lastName: string (nullable = true)
        |-- firstName: string (nullable = true)
        |-- age: integer (nullable = true)

* Eager and Lazy Execution
    ▪ Operations are eager when they are executed as soon as the statement is reached in the code
    ▪ Operations are lazy when the execution occurs only when the result is referenced
    ▪ Spark queries execute both lazily and eagerly
        ─ DataFrame schemas are determined eagerly
        ─ Data transformations are executed lazily
    ▪ Lazy execution is triggered when an action is called on a series of transformations

    - Example( p.164 )
        usersDF  = spark.read.json("users.json")





           
        




    




    







