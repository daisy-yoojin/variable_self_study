cloudera 
* certification
    - The leader in Apache Hadoop-based certification
    - Cloudera certification exams favor hands-on, performance-based problems 
        that require execution of a set of real-world tasks against a live, working cluster
    - Type
        > CCA
            CCA Spark and Hadoop Developer
            CCA Data Analyst
            CCA CDH Administrator and CCA HDP Administrator
        > CCP
            CCP Data Engineer
    
Introduction to Apache Hadoop and the Hadoop Ecosystem
    - 3V
        Volume, Velocity, Variety

Data Processing
    - Processing 
        apache Spark, Hadoop MapReduce
    - Resource Management
        YARN
    - Storage
        HDFS, HBase, Kudu, Cloud

Data Storage
    - HDFS
        main on-premises storage layer for Hadoop
        Inexpensive reliable storage for massive amounts of data
    - Apache HBase
        A NoSQL distributed database built on HDFS
    - Apache Kudu
        key-value storage
    - Cloud storage
        most hadoop ecosystem tools support in the cloude
        ex. Amazon S3, MS ADLS

Hadoop MapReduce: The Original Hadoop Processing Engine
    - the Original Hadoop framework for processing big data 
        Primarily Java-based
    - Still in use in many production systems

<<Apache Hadoop File Storage>>
* Hadoop Cluster Terminology
    - cluster is a group of computers working together
    - Master nodes manage distribution of work and data
    - Worker nodes store and process data
    - A daemon is a program running on a  node to provide a service

* HDFS Architecture

    1) Basic Concepts
    - HDFS is the original Hadoop storage system
    - Sits on top of a native file system
    - Provides redundant storage for massive amounts of data
    - HDFS performs best with a “modest” number of large files
    - Files in HDFS are “write once”
    - HDFS is optimized for large, streaming reads of files

    2) File be sotred
    - Data files are split into blocks (default 128MB) which are distributed at load time
    - Actual blocks are stored on cluster worker nodes running the Hadoop HDFS Data Node service
    - Each block is replicated on multiple DataNodes (default 3x)
    - A cluster master node runs the HDFS Name Node service, which stores file metadata
        Often referred to as the NameNode

                       block1    Data Node A
    large Data File -> block2 -> Data Node B  -> NameNode (Meta Info. about files and blocks)
                       block3    Data Node C
                       block4    Data Node D

* Using HDFS
    - Command line interface
        $hdfs dfs
    
*Essential Points
    - The Hadoop Distributed File System (HDFS) is the main storage layer for Hadoop
    - HDFS chunks data into blocks and distributes them to the cluster when data is stored
    - HDFS clusters consist of
        ─ A single NameNode to manage file metadata
        ─ Multiple DataNodes to store data
    - The hdfs dfs command allows you to use and manage files in HDFS

<<Distributed Processing on an Apache Hadoop Cluster>>

* YARN Architecture
    - What is YARN?
        - YARN = Yet Another Resource Negotiator
        - YARN is Hadoop processing layer (contains)
            Resource manager
            A job scheduler
    - YARN Daemons
        - Resource manager (RM)
            runs on master node
            global resource scheduler
            
        - Node manager (NM)
            runs on worker nodes
            communicates with RM
            Manages node resources
            Launches containers
        
        - containers
            allocate a certain amount of resources(memory, CPU cores) on a worker node
            Applications runs in one or more containers
            Applications request containers from RM

        - Application Master (AM)
            One per Application
            framework/application specific
            runs in a container
            request more containers to run application tasks

        -  Each application consists of one or more containers
            The ApplicationMaster runs in one container
            The application’s distributed processes (JVMs) run in other containers
            The processes run in parallel, and are managed by the AM
            The processes are called executors in Apache Spark and tasks in Hadoop MapReduce
        
        - Applications are typically submitted to the cluster from an edge or gateway node

* Essential Points
    - YARN manages resources in a Hadoop cluster and schedules Applications
    - Worker nodes run NodeManafer daemons, managed by a ResourceManager on a master node
    - Applications running on YARN consist of an ApplicationMaster and one or more executors
    - Use the YARN ResourceManager web UI or the yarn command to monitor Applications

<<Apache Spark Basics>>

* What is Apache Spark?
    - a fast, general-purpose engine for large-scale data processing
    - written in Scala
        Functional programming language that runs in a JVM
    - Spark Shell
        Interactive—for learning, data exploration, or ad hoc analytics
        Python and Scala
    - Spark Applications
        For large scale data processing
        Python, Scala, and Java
    
    - The Spark Stack
        ` Spark provides a stack of libraries built on core Spark
            ┌─────────┐ ┌─────────┐ ┌─────────┐ ┌─────────┐
            │  Spark  │ │   MLib  │ │  Spark  │ │  GraphX │ 
            │   SQL   │ │         │ │Streaming│ │         │
            └─────────┘ └─────────┘ └─────────┘ └─────────┘
            ┌─────────────────────────────────────────────┐
            │                 Core Spark                  │
            └─────────────────────────────────────────────┘
            Core Spark provides the fundamental Spark abstraction:
                Resilient Distributed Datasets (RDDs)
            Spark SQL works with structured data
            MLlib supports scalable machine learning
            Spark Streaming applications process data in real time
            GraphX works with graphs and graph-parallel computation
    
    - Spark SQL
        ` Spark SQL is a SPark library for working with structured data
        ` Spark SQL provide
            The DataFrame and Dataset API
                The primary entry point for developing Spark applications
                DataFrames adnd Datasets are abstractions for representing structured data
            Datalyst Optimizer - an extensible optimization framework
            A SQL engine and command line interface

* The Spark shell
    - The Spark shell provides an interactive Spark environment
    - typically run the Spark shell on a gateway node
    - on Cloudera cluster, the command to start Spark shell
        ` pyspark 
        ` spark-shell
    - start-up options
        ` master : specify the cluster to connect to
        ` jars : Additional JAR files
        ` py-files : Additional Python files (Python only)
        ` name : the name the Spark Application UI uses for this application
            Defaults to PySpark Shell or Spark shell
        ` help: Show all the available shell options
        ex) pyspark --name "My Application"

    - Spark Cluster options
        ` Spark applications can run on these types of clusters
            ─ Apache Hadoop YARN
            ─ Kubernetes
            ─ Apache Mesos
            ─ Spark Standalone
        ` They can also run locally instead of on a cluster
        ` CDH, HDP, and CDP Data Center use YARN
        ` CDP Private Cloud and CDP Public Cloud use Kubernetes
        ` Specify the type or URL of the cluster using the master option
        ` Set the master option to specify cluster type
            ─ yarn
            ─ local[*] runs locally with as many threads as cores (default)
            ─ local[n] runs locally with n threads
            ─ local runs locally with a single thread
        ex)
            pyspark --master yarn
            spark-shell --master yarn
    
    




    







