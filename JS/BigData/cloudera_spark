cloudera 
* certification
    - The leader in Apache Hadoop-based certification
    - Cloudera certification exams favor hands-on, performance-based problems 
        that require execution of a set of real-world tasks against a live, working cluster
    - Type
        > CCA
            CCA Spark and Hadoop Developer
            CCA Data Analyst
            CCA CDH Administrator and CCA HDP Administrator
        > CCP
            CCP Data Engineer
    
Introduction to Apache Hadoop and the Hadoop Ecosystem
    - 3V
        Volume, Velocity, Variety

Data Processing
    - Processing 
        apache Spark, Hadoop MapReduce
    - Resource Management
        YARN
    - Storage
        HDFS, HBase, Kudu, Cloud

Data Storage
    - HDFS
        main on-premises storage layer for Hadoop
        Inexpensive reliable storage for massive amounts of data
    - Apache HBase
        A NoSQL distributed database built on HDFS
    - Apache Kudu
        key-value storage
    - Cloud storage
        most hadoop ecosystem tools support in the cloude
        ex. Amazon S3, MS ADLS

Hadoop MapReduce: The Original Hadoop Processing Engine
    - the Original Hadoop framework for processing big data 
        Primarily Java-based
    - Still in use in many production systems

<<Apache Hadoop File Storage>>
* Hadoop Cluster Terminology
    - cluster is a group of computers working together
    - Master nodes manage distribution of work and data
    - Worker nodes store and process data
    - A daemon is a program running on a  node to provide a service

* HDFS Architecture

    1) Basic Concepts
    - HDFS is the original Hadoop storage system
    - Sits on top of a native file system
    - Provides redundant storage for massive amounts of data
    - HDFS performs best with a “modest” number of large files
    - Files in HDFS are “write once”
    - HDFS is optimized for large, streaming reads of files

    2) File be sotred
    - Data files are split into blocks (default 128MB) which are distributed at load time
    - Actual blocks are stored on cluster worker nodes running the Hadoop HDFS Data Node service
    - Each block is replicated on multiple DataNodes (default 3x)
    - A cluster master node runs the HDFS Name Node service, which stores file metadata
        Often referred to as the NameNode

                       block1    Data Node A
    large Data File -> block2 -> Data Node B  -> NameNode (Meta Info. about files and blocks)
                       block3    Data Node C
                       block4    Data Node D

* Using HDFS
    - Command line interface
        $hdfs dfs
    
*Essential Points
    - The Hadoop Distributed File System (HDFS) is the main storage layer for Hadoop
    - HDFS chunks data into blocks and distributes them to the cluster when data is stored
    - HDFS clusters consist of
        ─ A single NameNode to manage file metadata
        ─ Multiple DataNodes to store data
    - The hdfs dfs command allows you to use and manage files in HDFS

<<Distributed Processing on an Apache Hadoop Cluster>>

* YARN Architecture
    - What is YARN?
        - YARN = Yet Another Resource Negotiator
        - YARN is Hadoop processing layer (contains)
            Resource manager
            A job scheduler
    - YARN Daemons
        - Resource manager (RM)
            runs on master node
            global resource scheduler
            
        - Node manager (NM)
            runs on worker nodes
            communicates with RM
            Manages node resources
            Launches containers
        
        - containers
            allocate a certain amount of resources(memory, CPU cores) on a worker node
            Applications runs in one or more containers
            Applications request containers from RM

        - Application Master (AM)
            One per Application
            framework/application specific
            runs in a container
            request more containers to run application tasks

        -  Each application consists of one or more containers
            The ApplicationMaster runs in one container
            The application’s distributed processes (JVMs) run in other containers
            The processes run in parallel, and are managed by the AM
            The processes are called executors in Apache Spark and tasks in Hadoop MapReduce
        
        - Applications are typically submitted to the cluster from an edge or gateway node

* Essential Points
    - YARN manages resources in a Hadoop cluster and schedules Applications
    - Worker nodes run NodeManafer daemons, managed by a ResourceManager on a master node
    - Applications running on YARN consist of an ApplicationMaster and one or more executors
    - Use the YARN ResourceManager web UI or the yarn command to monitor Applications

<<Apache Spark Basics>>

* What is Apache Spark?
    - a fast, general-purpose engine for large-scale data processing
    - written in Scala
        Functional programming language that runs in a JVM
    - Spark Shell
        Interactive—for learning, data exploration, or ad hoc analytics
        Python and Scala
    - Spark Applications
        For large scale data processing
        Python, Scala, and Java
    
    - The Spark Stack
        ` Spark provides a stack of libraries built on core Spark
            ┌─────────┐ ┌─────────┐ ┌─────────┐ ┌─────────┐
            │  Spark  │ │   MLib  │ │  Spark  │ │  GraphX │ 
            │   SQL   │ │         │ │Streaming│ │         │
            └─────────┘ └─────────┘ └─────────┘ └─────────┘
            ┌─────────────────────────────────────────────┐
            │                 Core Spark                  │
            └─────────────────────────────────────────────┘
            Core Spark provides the fundamental Spark abstraction:
                Resilient Distributed Datasets (RDDs)
            Spark SQL works with structured data
            MLlib supports scalable machine learning
            Spark Streaming applications process data in real time
            GraphX works with graphs and graph-parallel computation
    
    - Spark SQL
        ` Spark SQL is a SPark library for working with structured data
        ` Spark SQL provide
            The DataFrame and Dataset API
                The primary entry point for developing Spark applications
                DataFrames adnd Datasets are abstractions for representing structured data
            Datalyst Optimizer - an extensible optimization framework
            A SQL engine and command line interface

* The Spark shell
    - The Spark shell provides an interactive Spark environment
    - typically run the Spark shell on a gateway node
    - on Cloudera cluster, the command to start Spark shell
        ` pyspark 
        ` spark-shell
    - start-up options
        ` master : specify the cluster to connect to
        ` jars : Additional JAR files
        ` py-files : Additional Python files (Python only)
        ` name : the name the Spark Application UI uses for this application
            Defaults to PySpark Shell or Spark shell
        ` help: Show all the available shell options
        ex) pyspark --name "My Application"

    - Spark Cluster options
        ` Spark applications can run on these types of clusters
            ─ Apache Hadoop YARN
            ─ Kubernetes
            ─ Apache Mesos
            ─ Spark Standalone
        ` They can also run locally instead of on a cluster
        ` CDH, HDP, and CDP Data Center use YARN
        ` CDP Private Cloud and CDP Public Cloud use Kubernetes
        ` Specify the type or URL of the cluster using the master option
        ` Set the master option to specify cluster type
            ─ yarn
            ─ local[*] runs locally with as many threads as cores (default)
            ─ local[n] runs locally with n threads
            ─ local runs locally with a single thread
        ex)
            pyspark --master yarn
            spark-shell --master yarn

    - Apache Zeppelin
        is a web-based notebook approach to interactive data analytics.
        Provides collaborative environment with Python, Scala, SQL, and more.

* Using Spark Shell
    - Working with the Spark Session
        provides functions and attributes to access all of Spark functionally
        ─ sql: execute a Spark SQL query
        ─ catalog: entry point for the Catalog API for managing tables
        ─ read: function to read data from a file or other data source
        ─ conf: object to manage Spark configuration settings
        ─ sparkContext: entry point for core Spark API

* Datasets and DataFrames
    - DataFrames contain a collection of Row objects
    - Creating DataFrame
        ` use spark.read
            ex) 
            # returns the Spark session's DataFrameReader
            # Call json dunction to create a new DataFrame
            usersDF = spark.read.json("users.json")

            # DataFrame always have an associated schema
            # DataFrameReader can infer the schema from the data
            # Use printSchema to show the DataFrame's schema

            # root
            # |-- age: long (nullable = true)
            # |-- name: string (nullable = true)
            # |-- pcode: string (nullable = true)
            usersDF.printSchema()

            # show
            # +----+-------+-----+
            # | age|   name|pcode|
            # +----+-------+-----+
            # |null|  Alice|94304|
            # |  30|Brayden|94304|
            # |  19|  Carla|10036|
            # |  46|  Diana| null|
            # |null|Etienne|94104|
            # +----+-------+-----+
            usersDF.show()

* DataFrame Operations
    - Two main types of DataFrame Operations
        ` Transformations create a new DataFrame based on existing one 
            Transformations are executed in parallel by the application's executors
        ` Actions output data values from the DataFrame
            Output is typically returned from the executors to the main Spark program (called the driver) or saved to a file
    - Some common DataFrame actions include
        ─ count: returns the number of rows
        ─ first: returns the first row (synonym for head())
        ─ take(n): returns the first n rows as an array (synonym for head(n))
        ─ show(n): display the first n rows in tabular form (default is 20 rows)
        ─ collect: returns all the rows in the DataFrame as an array
        ─ write: save the data to a file or other data source

            # Python ver.
            usersDF = spark.read.json("users.json")
            users = usersDF.take(3)
            
            ### result
            [Row(age=None, name=u'Alice', pcode=u'94304'),
            Row(age=30, name=u'Brayden', pcode=u'94304'),
            Row(age=19, name=u'Carla', pcode=u'10036')]

            # Scala ver.
            val usersDF = spark.read.json("users.json")
            val users = usersDF.take(3)
           
            ### result
            usersDF: Array[org.apache.spark.sql.Row] =
            Array([null,Alice,94304],
            [30,Brayden,94304],
            [19,Carla,10036])

    - DataFrame Operations: Transformations ★★★★★
        ` Transformations create a new DataFrame based on an existing one
            ─ The new DataFrame may have the same schema or a different one
        ` Transformations do not return any values or data to the driver
            ─ Data remains distributed across the application’s executors
        ` DataFrames are immutable
            ─ Data in a DataFrame is never modified
            ─ Use transformations to create a new DataFrame with the data you need  
        ` Common transformations include
            ─ select: only the specified columns are included
            ─ where: only rows where the specified expression is true are included (synonym for filter)
            ─ orderBy: rows are sorted by the specified column(s) (synonym for sort)
            ─ join: joins two DataFrames on the specified column(s)
            ─ limit(n): creates a new DataFrame with only the first n rows

            # Python
            nameAgeDF = usersDF.select("name","age")
            nameAgeDF.show()
            # result
            +-------+----+
            |   name| age|
            +-------+----+
            |  Alice|null|
            |Brayden|  30|
            |  Carla|  19|
            |  Diana|  46|
            |Etienne|null|
            +-------+----+
            over20DF = usersDF.where("age > 20")
            over20DF.show()
            +---+-------+-----+
            |age|   name|pcode|
            +---+-------+-----+
            | 30|Brayden|94304|
            | 46|  Diana| null|
            +---+-------+-----+

        ` A sequence of transformations followed by an action is a query
            nameAgeDF = usersDF.select("name","age")
            nameAgeOver20DF = nameAgeDF.where("age > 20")
            nameAgeOver20DF.show()
            +---+-------+
            |age|   name|
            +---+-------+
            | 30|Brayden|
            | 46|  Diana|
            +---+-------+
    
    *** Chaining Transformations
        ` Transformations in a query can be chained together
        `  These two examples perform the same query in the same way
            ─ Differences are only syntactic

            # Python ver.
            # 1 
            nameAgeDF = usersDF.select("name","age")
            nameAgeOver20DF = nameAgeDF.where("age > 20")
            nameAgeOver20DF.show()
            # 2
            usersDF.select("name","age").where("age > 20").show()

            # Scala ver.
            #1
            val nameAgeDF = usersDF.select("name","age")
            val nameAgeOver20DF = nameAgeDF.where("age > 20")
            nameAgeOver20DF.show            
            #2
            usersDF.select("name","age").where("age > 20").show

* Essential Points
    - Apache Spark is a framework for analyzing and processing big data
    - The Python and Scala Spark shells are command line REPLs for executing Spark interactively
        ─ Spark applications run in batch mode outside the shell
    - DataFrames represent structured data in tabular form by applying a schema
    - Types of DataFrame operations
        ─ Transformations create new DataFrames by transforming data in existing ones
        ─ Actions collect values in a DataFrame and either save them or return them to the Spark driver
    - A query consists of a sequence of transformations followed by an action

-p.137
<< Working with DataFrames and Schema >>










           
        




    




    







